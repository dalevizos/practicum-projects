{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying patterns that determine whether a game succeeds or not\n",
    "\n",
    "This Project is for an online store that sells video games all over the world. They have provided us with a variety of data such as; user and expert reviews, genres, platforms (e.g. Xbox or PlayStation), and historical data on game sales. Based on the above data we will have to identify patterns that can potentially determine whether a game succeeds or not. It's important to note the dataset contains the abbreviation ESRB, which stands for Entertainment Software Rating Board - evaluates a game's content and assigns an age rating such as Teen or Mature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Information\n",
    "\n",
    "We start our project by loading the required libraries that we will need to complete future tasks. Additionally, we open our dataset and have a first glance in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#needed for mathematical operations\n",
    "\n",
    "import pandas as pd\n",
    "#needed for dataframe manipulation\n",
    "\n",
    "import seaborn as sns \n",
    "#needed for data visualization\n",
    "\n",
    "from scipy import stats as st\n",
    "#needed for statistical analysis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#needed for data visualization\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings (\"ignore\")\n",
    "#needed to ignore future warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/datasets/games.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m/datasets/games.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[39m#loading our dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    663\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    664\u001b[0m     dialect,\n\u001b[1;32m    665\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    675\u001b[0m )\n\u001b[1;32m    676\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    931\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1217\u001b[0m     f,\n\u001b[1;32m   1218\u001b[0m     mode,\n\u001b[1;32m   1219\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1220\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1221\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1223\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1224\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1225\u001b[0m )\n\u001b[1;32m   1226\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    787\u001b[0m             handle,\n\u001b[1;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/datasets/games.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/datasets/games.csv\")\n",
    "#loading our dataset\n",
    "\n",
    "df.shape\n",
    "#getting the dataset's number of rows and columns \n",
    "\n",
    "df.info()\n",
    "#getting the main info from our dataset\n",
    "\n",
    "df.head()\n",
    "#printing the first 5 rows of our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Conclusion\n",
    "\n",
    "We can identify a variety of problems with our current dataset. We will have to do the following:\n",
    "\n",
    "- Convert the data to the required types. \n",
    "- Replace the column names (make them all lowercase)\n",
    "- Calculate total sales for each game and add these values in a separate column.\n",
    "- Find a way to efficiently deal with all the missing values after calculating the % for each column's missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower()\n",
    "#converting all columns to lower case letters\n",
    "\n",
    "(df.isnull().sum()/len(df)).round(4)\n",
    "#getting the % of missing values for each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platform Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['platform'] = df['platform'].str.lower()\n",
    "#making all letters in the column lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column has no missing values. We just converted all letters to lower case to simplify our future tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['name'].isnull()] )\n",
    "#printing the null rows in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name'] = df['name'].str.lower()\n",
    "#making all letters in the column lower case\n",
    "\n",
    "df['name'].unique()\n",
    "#getting the unique names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we notice that mario kart includes the platform name on its name and probably that is not meant to be that way. Just in case we will proceed based on the assumption that this can be seen in a variety of other games too so we will create a function that will take a row as an argument and will be checking if the name includes the platform name as well, in the scenario that it does we will automate the platform name to be removed from the name. We will use the try / except because we have 2 missing values so the function will only operate if and when the value is not na."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_platform_name (row):\n",
    "    #naming our function'removing_platform_name' and passing row as argument\n",
    "    \n",
    "    try:\n",
    "        name = row['name']\n",
    "        #storing the row['name'] value into a variable called name\n",
    "        \n",
    "        if row['platform'] in row ['name']:\n",
    "        #if the 'platform name' is seen in the 'name' variable\n",
    "        \n",
    "            name = row['name'].strip(row['platform'])\n",
    "            #then we use the strip() method to remove it from the name\n",
    "            \n",
    "            return name \n",
    "            #returning new name\n",
    "\n",
    "        else:\n",
    "        #if the 'platform name' is not seen in the 'name' variable \n",
    "        \n",
    "            return name \n",
    "            #will simply return the current name\n",
    "        \n",
    "    except:\n",
    "        #this applies only when there're missing values\n",
    "        \n",
    "        name = row['name']\n",
    "        #storing the row['name'] value into a variable called name\n",
    "\n",
    "        return name \n",
    "        #will simply return the current name\n",
    "\n",
    "df['name'] = df.apply(removing_platform_name, axis=1)\n",
    "#applying the function above to our dataset's column 'name'\n",
    "\n",
    "df.head()\n",
    "#printing the first 5 rows of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name'] = df['name'].dropna()\n",
    "#dropping the null rows in the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there's only two missing values in this column we will drop them. It's either values that were never entered or entered incorrectly. We converted all letters to lower case to simplify our future tasks. Other than the comments stated above the column looks good with no additional steps required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year of Release Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year_of_release'].isnull().sum() \n",
    "#printing the number of null rows in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year_of_release'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_median_year = df.groupby('platform')['year_of_release'].median().to_dict()\n",
    "# grouping our dataframe by platform to find the median year_of_release per platform\n",
    "# storing the resulting dataframe into a dictionary named grouped_median_year\n",
    "\n",
    "def filling_missing_values (row):\n",
    "    #creating a function called filling_missing_values that takes a variable 'row' as its argument\n",
    "\n",
    "    if np.isnan(row['year_of_release']):\n",
    "    #if the value of year_of_release column is missing\n",
    "\n",
    "        return grouped_median_year.get(row['platform'])\n",
    "        #returning the median year_of_release value based on the corresponding platform \n",
    "    \n",
    "    else:\n",
    "    #if the value of year_of_release column is not missing\n",
    "  \n",
    "        return row['year_of_release']\n",
    "        #will return its existing value\n",
    "\n",
    "df['year_of_release'] = df.apply(filling_missing_values, axis=1)\n",
    "#applying the filling_missing_values function to the year_of_release column in our dataframe\n",
    "\n",
    "df.info()\n",
    "#confirming the above changes \n",
    "\n",
    "df['year_of_release'].isnull().sum() \n",
    "#printing the number of null rows in the column, now should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 'year_of_release' column we had 1.61% missing values and decided to replace them with the median. However it wouldn't be accurate to replace them by the total median and thus we decided to first create a dictionary with the median per platform and then replace missing values according their platform, that way would be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genre Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genre'] = df['genre'].str.lower()\n",
    "#making all letters in the column lower case\n",
    "\n",
    "df['genre'].unique()\n",
    "#getting the unique names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genre'].isnull().sum() \n",
    "#printing the number of null rows in the column\n",
    "\n",
    "df['genre'] = df['genre'].dropna()\n",
    "#dropping the null rows in the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column has only 2 missing values and thus we will leave it as it is. We just converted all letters to lower case to simplify our future tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic Score Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['critic_score'].isnull().sum() \n",
    "#printing the number of null rows in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['critic_score'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have 51% of our data being missing values. We can't replace them by the median/mean since is very individual as every critic has own oppinion on the game. If we just NaNs with some mean (even based on other column) it can spoil our final result as critic score will not be representative anymore. We will replace with a dummy value that is not pragmatice such as -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['critic_score'] = df['critic_score'].fillna(value=-1)\n",
    "#filling all missing values with -1 since is not a pragmatic value\n",
    "\n",
    "df['critic_score'].isnull().sum() \n",
    "#printing the number of null rows in the column, now should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 'critic_score' column we had 51% missing values which we replaced by a dummy value that is not pragmatice such as -1. We couldn't replace them by the median/mean since is very individual as every critic has own oppinion on the game. Now our column is good for analysis.\n",
    "\n",
    "Lastly, we can conclude that the missing values are probably due to the fact that not all critics submit a review for each game. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Score Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_score'].unique()\n",
    "#printing all the unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we noticed that the column is an object type column even though it should have been float64. So we decided to take a closer look at the unique values to determine what make this column an object type column. We see that there's a value 'tbd' (to be determined) this refers to data that we are expecting to have but haven't gotten yet thus we will need to replace with na and convert column to float. Later, we will need to follow the same strategy as above to replace column's missing values by taking the median user_score per genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_score'] = df['user_score'].fillna(value=-1)\n",
    "#filling all missing values with -1 since is not a pragmatic value\n",
    "\n",
    "df['user_score'] = df['user_score'].replace(\"tbd\", \"-1\")\n",
    "#replacing all \"tbd\" values with -1 since is not a pragmatic value\n",
    "\n",
    "df['user_score'] = pd.to_numeric(df['user_score'])\n",
    "#convertingd column's data type from string to numeric\n",
    "\n",
    "df['user_score'].isnull().sum() \n",
    "#printing the number of null rows in the column, now should be 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_score'].unique()\n",
    "#printing all the unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our column is \"clean\" and ready for analysis!\n",
    "\n",
    "Lastly, we can conclude that the missing values are probably due to the fact that not all users submit a review. We can think all the times when we were personally asked to write a review for a product and instead we politely declined. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 40% of the values in this column are missing values and we have to take care of them but first let's take a closer look at the unique values to understand what they stand for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'].unique()\n",
    "#printing all the unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'] = df['rating'].fillna(value=\"no info\")\n",
    "#filling all missing values with -1 since is not a pragmatic value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'].unique()\n",
    "#confirming all changes have been made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These ESRB ratings (Enetrtainment Software Rating Board) are the signs that we see on the game's packaging, for example 'E' stands for Everyone, 'E10+' stands for everyone 10+ etc. We will leave the missing values as they are.  Since that was the last column we needed to \"correct\" now our dataset looks ready for analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Total Sales Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_sales'] = df['na_sales'] + df['eu_sales'] + df['jp_sales'] + df['other_sales']\n",
    "#adding the new column of total sales by summing the sales of all regions \n",
    "\n",
    "df.info()\n",
    "#new dataset's information\n",
    "\n",
    "df.head()\n",
    "#confirming the above changes  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now cleaned our data and we are ready to start our data analysis based on reliable data that we can count on. Before moving forward let's quickly summarize all the steps we have taken thus far:\n",
    "\n",
    "- We edited the names for each game so that it wouldn't include the platform name as well\n",
    "- We have updated all columns to their correct data type (such as column: user_score to float)\n",
    "- We have replaced all missing values with the corresponding median or moder (after first grouping the data according to the each column's conidtion)\n",
    "- Lastly, we added additional column to our dataset such as the total_sales by summing all sales of all regions. So now we can easily the total sales per game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Games Released per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_count = df.groupby('year_of_release')['name'].count()\n",
    "# grouping our dataframe by year_of_release to find the amount of games released that year\n",
    "\n",
    "year_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_count.plot(kind='bar', figsize=(24, 10))\n",
    "#creating a bar chart\n",
    "\n",
    "plt.title(\"Number of Games Released per Year\")\n",
    "#having a title to the chart\n",
    "\n",
    "plt.xlabel(\"Year/s\")\n",
    "#naming x axis\n",
    "\n",
    "plt.ylabel(\"Number of Games\")\n",
    "#naming y axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above graph we can clearly identify an exponential growth of game releases between the years of 2007 and 2011, with the pick being in 2009 with approximately 1500 games release. Thus not all periods are significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platform Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform_sales = df.groupby('platform')['total_sales'].sum().sort_values(ascending=False)\n",
    "#grouping our dataframe by platform to find its total amount of sales\n",
    "\n",
    "top_platforms = platform_sales.head()\n",
    "#top 5 platforms\n",
    "\n",
    "top_platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can identiffy a clear winner, which is ps2 with more than 1.2B, followed by x360, ps3, wii and ds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps2_data = df[df['platform'] == 'ps2']\n",
    "#storing all ps2 data to a new variable\n",
    "\n",
    "ps2_sales = ps2_data.groupby('year_of_release')['total_sales'].count()\n",
    "#grouping our new dataframe by year/s to find its total amount of sales\n",
    "\n",
    "ps2_sales.plot(kind='bar', figsize=(24, 10))\n",
    "#creating a bar chart\n",
    "\n",
    "plt.title(\"Total Sales of PS2 Throughout the Years\")\n",
    "#having a title to the chart\n",
    "\n",
    "plt.xlabel(\"Year/s\")\n",
    "#naming x axis\n",
    "\n",
    "plt.ylabel(\"Sales\")\n",
    "#naming y axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x360_data = df[df['platform'] == 'x360']\n",
    "#storing all x360 data to a new variable\n",
    "\n",
    "x360_sales = x360_data.groupby('year_of_release')['total_sales'].count()\n",
    "#grouping our new dataframe by year/s to find its total amount of sales\n",
    "\n",
    "x360_sales.plot(kind='bar', figsize=(24, 10))\n",
    "#creating a bar chart\n",
    "\n",
    "plt.title(\"Total Sales of x360 Throughout the Years\")\n",
    "#having a title to the chart\n",
    "\n",
    "plt.xlabel(\"Year/s\")\n",
    "#naming x axis\n",
    "\n",
    "plt.ylabel(\"Sales\")\n",
    "#naming y axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps3_data = df[df['platform'] == 'ps3']\n",
    "#storing all ps3 data to a new variable\n",
    "\n",
    "ps3_sales = ps3_data.groupby('year_of_release')['total_sales'].count()\n",
    "#grouping our new dataframe by year/s to find its total amount of sales\n",
    "\n",
    "ps3_sales.plot(kind='bar', figsize=(24, 10))\n",
    "#creating a bar chart\n",
    "\n",
    "plt.title(\"Total Sales of PS3 Throughout the Years\")\n",
    "#having a title to the chart\n",
    "\n",
    "plt.xlabel(\"Year/s\")\n",
    "#naming x axis\n",
    "\n",
    "plt.ylabel(\"Sales\")\n",
    "#naming y axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wii_data = df[df['platform'] == 'wii']\n",
    "#storing all wii data to a new variable\n",
    "\n",
    "wii_sales = wii_data.groupby('year_of_release')['total_sales'].count()\n",
    "#grouping our new dataframe by year/s to find its total amount of sales\n",
    "\n",
    "wii_sales.plot(kind='bar', figsize=(24, 10))\n",
    "#creating a bar chart\n",
    "\n",
    "plt.title(\"Total Sales of wii Throughout the Years\")\n",
    "#having a title to the chart\n",
    "\n",
    "plt.xlabel(\"Year/s\")\n",
    "#naming x axis\n",
    "\n",
    "plt.ylabel(\"Sales\")\n",
    "#naming y axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_data = df[df['platform'] == 'ds']\n",
    "#storing all ds data to a new variable\n",
    "\n",
    "ds_sales = ds_data.groupby('year_of_release')['total_sales'].count()\n",
    "#grouping our new dataframe by year/s to find its total amount of sales\n",
    "\n",
    "ds_sales.plot(kind='bar', figsize=(24, 10))\n",
    "#creating a bar chart\n",
    "\n",
    "plt.title(\"Total Sales of ds Throughout the Years\")\n",
    "#having a title to the chart\n",
    "\n",
    "plt.xlabel(\"Year/s\")\n",
    "#naming x axis\n",
    "\n",
    "plt.ylabel(\"Sales\")\n",
    "#naming y axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to create graph charts for each of the 5 most common platforms, ps2, x360, ps3, wii and ds. All the graphs above show the ditribution of sales throughout the year per platform. After taking a closer look we can identify:\n",
    "\n",
    "ps2: 10 years of sales period from 2000 to 2011 (approximately 5 years dominated the market from 2002 to 2006)\n",
    "\n",
    "x360: 12 years of sales period from 2005 to 2016 (approximately 2 years dominated the market from 2010 to 2011)\n",
    "\n",
    "ps3: 11 years of sales period from 2006 to 2016 (approximately 1 year dominated the market in 2011)\n",
    "\n",
    "wii: 11 years of sales period from 2006 to 2016 (approximately 3 years dominated the market from 2008 to 2010)\n",
    "\n",
    "ds: 11 years of sales period from 2004 to 2013 (approximately 1 year dominated the market from 2007 to 2011)\n",
    "\n",
    "We can identify that most game platforms last around 11 years. Additionally, they are being popular (dominating the market) for approximately 4/5 years before they start fading out. After taking a closer look at the graphs above we determine to keep data for the last 4 years to efficiently build a prognosis for the year of 2017. We choose 4 years because game market is very dynamic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platform Performance after 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[df['year_of_release'] >= 2013.0]\n",
    "#storing data from and after the year of 2007 to a new variable called 'new_df'\n",
    "\n",
    "platform_sales2 = new_df.groupby('platform')['total_sales'].sum().sort_values(ascending=False)\n",
    "#grouping our dataframe by platform to find its total amount of sales\n",
    "\n",
    "top_platforms2 = platform_sales2.head(10)\n",
    "#top 10 platforms\n",
    "\n",
    "top_platforms2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 2013 ps3 is the platform with the most sales, specifically 918M and x360 comes second with 911M and third we have wii with 770M. We can see that choosing the more recent yet relevant data changes our top platforms. We previously had ps2 in number 1 and now ps2 is number 9, clearly ps2 sales are shrinking potentially because the new versions have entered the market such as ps3 and ps4. Moving forward we are going to keep the top 5 in the above list, ps3, x360, wii, ds and ps4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ps3_data = new_df[new_df['platform'] == 'ps3']\n",
    "#storing all ps3 data to a new variable\n",
    "\n",
    "new_ps3_sales = new_ps3_data.groupby('year_of_release')['total_sales'].count()\n",
    "#grouping our new dataframe by year/s to find its total amount of sales\n",
    "\n",
    "new_ps3_sales.plot(kind='bar', figsize=(24, 10))\n",
    "#creating a bar chart\n",
    "\n",
    "plt.title(\"Total Sales of PS3 Since 2013\")\n",
    "#having a title to the chart\n",
    "\n",
    "plt.xlabel(\"Year/s\")\n",
    "#naming x axis\n",
    "\n",
    "plt.ylabel(\"Sales\")\n",
    "#naming y axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x360_data = new_df[new_df['platform'] == 'x360']\n",
    "#storing all x360 data to a new variable\n",
    "\n",
    "new_x360_sales = new_x360_data.groupby('year_of_release')['total_sales'].count()\n",
    "#grouping our new dataframe by year/s to find its total amount of sales\n",
    "\n",
    "new_x360_sales.plot(kind='bar', figsize=(24, 10))\n",
    "#creating a bar chart\n",
    "\n",
    "plt.title(\"Total Sales of x360 Since 2013\")\n",
    "#having a title to the chart\n",
    "\n",
    "plt.xlabel(\"Year/s\")\n",
    "#naming x axis\n",
    "\n",
    "plt.ylabel(\"Sales\")\n",
    "#naming y axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wii_data = new_df[new_df['platform'] == 'wii']\n",
    "#storing all wii data to a new variable\n",
    "\n",
    "new_wii_sales = new_wii_data.groupby('year_of_release')['total_sales'].count()\n",
    "#grouping our new dataframe by year/s to find its total amount of sales\n",
    "\n",
    "new_wii_sales.plot(kind='bar', figsize=(24, 10))\n",
    "#creating a bar chart\n",
    "\n",
    "plt.title(\"Total Sales of wii Since 2013\")\n",
    "#having a title to the chart\n",
    "\n",
    "plt.xlabel(\"Year/s\")\n",
    "#naming x axis\n",
    "\n",
    "plt.ylabel(\"Sales\")\n",
    "#naming y axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds_data = new_df[new_df['platform'] == 'ds']\n",
    "#storing all ds data to a new variable\n",
    "\n",
    "new_ds_sales = new_ds_data.groupby('year_of_release')['total_sales'].count()\n",
    "#grouping our new dataframe by year/s to find its total amount of sales\n",
    "\n",
    "new_ds_sales.plot(kind='bar', figsize=(24, 10))\n",
    "#creating a bar chart\n",
    "\n",
    "plt.title(\"Total Sales of ds Since 2013\")\n",
    "#having a title to the chart\n",
    "\n",
    "plt.xlabel(\"Year/s\")\n",
    "#naming x axis\n",
    "\n",
    "plt.ylabel(\"Sales\")\n",
    "#naming y axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here for ds we don't have any year other than 2013 since we kept data from 2013 and onwards and 2013 was was the last year of sales for the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ps4_data = new_df[new_df['platform'] == 'ps4']\n",
    "#storing all ps4 data to a new variable\n",
    "\n",
    "new_ps4_sales = new_ps4_data.groupby('year_of_release')['total_sales'].count()\n",
    "#grouping our new dataframe by year/s to find its total amount of sales\n",
    "\n",
    "new_ps4_sales.plot(kind='bar', figsize=(24, 10))\n",
    "#creating a bar chart\n",
    "\n",
    "plt.title(\"Total Sales of PS4 Since 2013\")\n",
    "#having a title to the chart\n",
    "\n",
    "plt.xlabel(\"Year/s\")\n",
    "#naming x axis\n",
    "\n",
    "plt.ylabel(\"Sales\")\n",
    "#naming y axis\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After visualizing the sales of the top 5 platforms since 2013 we can identify that wii sales start shrinking after 2013. Ds stops having any sales after the year 2013. x360 sales start shrinking after 2013 and ps3 sales start slowing down after 2015. Lastly ps4 seems to be the most promising due to the fact that is also the newest platform so we can see that it entered the market in 2013 and from our previous analysis we predict that will stay popular for approximately 10/11 years in total so for about the next 6/7 years. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plot for the Total Sales of all Games for the top 5 platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by plotting all the total sales per platform from our new_df variable, the data which we conclude that is the most relevant data. After getting a first glance at it then we will specifically analyze each single platform of our top 5 so, ps3, x360, wii, ds and ps4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.boxplot(column='total_sales', by='platform', figsize=(15,20))\n",
    "#plotting a boxplot for the total_sales column that has size 15 x 10 inches\n",
    "\n",
    "plt.show()\n",
    "#showing the boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ps3_data.boxplot(column='total_sales', by='platform', figsize=(15,10))\n",
    "#plotting a boxplot for the total_sales column of ps3 that has size 15 x 10 inches\n",
    "\n",
    "plt.show()\n",
    "#showing the boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x360_data.boxplot(column='total_sales', by='platform', figsize=(15,10))\n",
    "#plotting a boxplot for the total_sales column of x360 that has size 15 x 10 inches\n",
    "\n",
    "plt.show()\n",
    "#showing the boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wii_data.boxplot(column='total_sales', by='platform', figsize=(15,10))\n",
    "#plotting a boxplot for the total_sales column of wii that has size 15 x 10 inches\n",
    "\n",
    "plt.show()\n",
    "#showing the boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds_data.boxplot(column='total_sales', by='platform', figsize=(15,10))\n",
    "#plotting a boxplot for the total_sales column of ds that has size 15 x 10 inches\n",
    "\n",
    "plt.show()\n",
    "#showing the boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ps4_data.boxplot(column='total_sales', by='platform', figsize=(15,10))\n",
    "#plotting a boxplot for the total_sales column of ps4 that has size 15 x 10 inches\n",
    "\n",
    "plt.show()\n",
    "#showing the boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can identify no major difference between the boxplots. All medians are close to 0 however we notice extreme outliers for every platform which represent the games that were extremelly successful. \n",
    "\n",
    "PS3 & x360 platforms:\n",
    "More specifically, for ps3 and x360 (have very similar data) we notice that all 4 quartiles are approximately 2 which means that all of the games made a total profit of around 2M, however there's a lot of outliers, which means that there's been a variety of games that had sales well above the max, lots of ourliers ate between 5-10M and there's even some as high as 22M.\n",
    "\n",
    "wii platform:\n",
    "Wii data differs from the one analyzed above in the sense that we have all 4 quartiles to be around 0.5M however, there're many extreme outliers. For example we have outliers as high as 35M! \n",
    "\n",
    "ds platform & ps4 platforms:\n",
    "For ds and ps4 platforms (have very similar data), we see a similar pattern; lots of outliers and a median close to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User & Critic Reviews vs Sales for Nintendo DS Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds_data.plot(x='user_score', y='total_sales', kind='scatter', figsize=(15,5))\n",
    "#plotting a scatter for the new_ds_data with x column user_score and y column the total_sales, size 15 x 5 inches\n",
    "\n",
    "plt.show()\n",
    "#showing the scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds_data.plot(x='critic_score', y='total_sales', kind='scatter', figsize=(15,5))\n",
    "#plotting a scatter for the new_ds_data with x column critic_score and y column the total_sales, size 15 x 5 inches\n",
    "\n",
    "plt.show()\n",
    "#showing the scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to choose the Nintendo Developer's System platform (ds) to analyze further the potential correlation between the user_score / critic_score and the total_sales. Overall we do see visually that the higher the rate of either score the higher the sales will be. More specififcally, we identify visually the user_score to have lots of sales when the score is > 7 and for the critic_score to have lots of success when the score is above > 65. Let's dig deeper by actually performing the correlation coefficient method and see if what we identify visually can be justified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (new_ds_data['user_score'].corr(new_ds_data['total_sales']))\n",
    "#finding the correlation coefficient between user_score and total_sales for ds\n",
    "\n",
    "print (new_ds_data['critic_score'].corr(new_ds_data['total_sales']))\n",
    "#finding the correlation coefficient between critic_score and total_sales for ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the correlation method for both the user_score and the critic_score we can identify NO correlation between either score and total_sales. Now let's compare the same game sales to other platforms and see if any of the correlation changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (new_ps3_data['user_score'].corr(new_ps3_data['total_sales']))\n",
    "#finding the correlation coefficient between user_score and total_sales for ps3\n",
    "\n",
    "print (new_ps3_data['critic_score'].corr(new_ps3_data['total_sales']))\n",
    "#finding the correlation coefficient between critic_score and total_sales for ps3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (new_x360_data['user_score'].corr(new_x360_data['total_sales']))\n",
    "#finding the correlation coefficient between user_score and total_sales for x360\n",
    "\n",
    "print (new_x360_data['critic_score'].corr(new_x360_data['total_sales']))\n",
    "#finding the correlation coefficient between critic_score and total_sales for x360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (new_wii_data['user_score'].corr(new_wii_data['total_sales']))\n",
    "#finding the correlation coefficient between user_score and total_sales for x360\n",
    "\n",
    "print (new_wii_data['critic_score'].corr(new_wii_data['total_sales']))\n",
    "#finding the correlation coefficient between critic_score and total_sales for x360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (new_ps4_data['user_score'].corr(new_ps4_data['total_sales']))\n",
    "#finding the correlation coefficient between user_score and total_sales for ps4\n",
    "\n",
    "print (new_ps4_data['critic_score'].corr(new_ps4_data['total_sales']))\n",
    "#finding the correlation coefficient between critic_score and total_sales for ps4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall we don't really see any strong correlation coefficient in any platform. However, we can identify a moderate correlation coefficient between the critic_score and total_sales for the platforms of ps4, x360 and ps3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the Most Profitable Genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first identify the amount of games distribution per genre and later we will identify the genres with high and low sales by using the groupby. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_count = new_df.groupby('genre')['total_sales'].count().sort_values(ascending=False)\n",
    "#collecting all genres and their distribution\n",
    "\n",
    "genre_count\n",
    "#printing genres_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_count.plot(kind='bar', figsize=(15,5))\n",
    "#plotting a bar plot for the genres distribution with size 15 x 5 inches\n",
    "\n",
    "plt.show()\n",
    "#showing the bar plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that most of the games that have been distributed in our data belong to the action genre. At second place we have misc (a game developer that aims at developing unique games for various niche markets) and third we have sports. Later, we will analyze the total sales per genre and it will be interesting to see how the total sales per genre compare to the total distribution per genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_sales = new_df.groupby('genre')['total_sales'].sum().sort_values(ascending=False)\n",
    "#collecting all genre_sales in descending order\n",
    "\n",
    "genre_sales\n",
    "#printing genre_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_sales.plot(kind='bar', figsize=(15,5))\n",
    "#plotting a bar plot for the genre sales with size 15 x 5 inches\n",
    "\n",
    "plt.show()\n",
    "#showing the bar plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the general distribution of dames by Genre in terms of total_sales by creating a boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='total_sales', by='genre', figsize=(15,10))\n",
    "#plotting a boxplot for the total_sales by genre of all platforms that has size 15 x 10 inches\n",
    "\n",
    "plt.show()\n",
    "#showing the boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At fist glance we can identify no major difference between the boxplots. All medians are close to 0 however we notice extreme outliers for every genre which represent the games that were extremelly successful. More specifically, platform, racing, and role-playing.\n",
    "\n",
    "\n",
    "Above we analyzed the total sales of games per genre and we have at first spot the action genre, seocnd the shooter genre and lastly the sports genre. It's important to note that we previously when we looked at the genre distribution we didn't have the shooter at top 5 but in total_sales comes second! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our analysis we looked at the games release per year and and the perfomance for each platform. Later on, we decided to slice our data for data after 2007 since that would be the most relevant for the purpose of our project. After, we created a box plot for the total sales of all games we took a closer look for our top 5 platforms, ps3, x360, wii, ds and ps4. Later, we tried to find a possible correlation coefficient for the the user / critic reviews and total sales for all 5 platforms and we concluded that none had enough evidence to provide a strong correlation. Lastly, we identified the most profitable genres and also looked at all of their distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Profile for Each Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step of our project we ephasize on the data per region. We have three regions, North America, Europe and Japana and thus we will create three new variables to store their corresponding relevant data. We will then identify the top 5 platforms per region and how they might differ. Later on we will follow the same strategy for the top 5 genre per region and lastly we will try to find a potential correlation between the ESRB ratings and the sales in individual regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_region = new_df[['name', 'platform', 'year_of_release', 'genre', 'na_sales', 'critic_score', 'user_score', 'rating']]\n",
    "#storing all relevant data to a new variable for the north american region\n",
    "\n",
    "eu_region = new_df[['name', 'platform', 'year_of_release', 'genre', 'eu_sales', 'critic_score', 'user_score', 'rating']]\n",
    "#storing all relevant data to a new variable for the european region\n",
    "\n",
    "jp_region = new_df[['name', 'platform', 'year_of_release', 'genre', 'jp_sales', 'critic_score', 'user_score', 'rating']]\n",
    "#storing all relevant data to a new variable for the japan region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platform Performance per Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Platform Performance in North America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_platform_sales = na_region.groupby('platform')['na_sales'].sum().sort_values(ascending=False)\n",
    "#grouping our na_region dataframe by platform to find its total amount of sales for the north america region\n",
    "\n",
    "na_platform_sales_top5 = na_platform_sales.head()\n",
    "#storing the top 5 platforms to a variable called na_platform_sales_top5\n",
    "\n",
    "na_platform_sales_top5.plot(kind='bar', figsize=(15,5))\n",
    "#plotting the top 5 platforms\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.pie(na_platform_sales_top5, labels=na_platform_sales_top5.index, radius=3)\n",
    "#plotting in pie chart ALL platforms\n",
    "\n",
    "plt.title(\"Top 5 Platforms in North America since 2012 \")\n",
    "#naming the pie chart\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Platform Performance in Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_platform_sales = eu_region.groupby('platform')['eu_sales'].sum().sort_values(ascending=False)\n",
    "#grouping our eu_platform_sales dataframe by platform to find its total amount of sales for the european region\n",
    "\n",
    "eu_platform_sales_top5 = eu_platform_sales.head()\n",
    "#storing the top 5 platforms to a variable called eu_platform_sales_top5\n",
    "\n",
    "eu_platform_sales_top5.plot(kind='bar', figsize=(15,5))\n",
    "#plotting the top 5 platforms\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.pie(eu_platform_sales_top5, labels=eu_platform_sales_top5.index, radius=3)\n",
    "#plotting in pie chart ALL platforms\n",
    "\n",
    "plt.title(\"Top 5 Platforms in Europe since 2012 \")\n",
    "#naming the pie chart\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Platform Performance in Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_platform_sales = jp_region.groupby('platform')['jp_sales'].sum().sort_values(ascending=False)\n",
    "#grouping our jp_platform_sales dataframe by platform to find its total amount of sales for the japan region\n",
    "\n",
    "jp_platform_sales_top5 = jp_platform_sales.head()\n",
    "#storing the top 5 platforms to a variable called eu_platform_sales_top5\n",
    "\n",
    "jp_platform_sales_top5.plot(kind='bar', figsize=(15,5))\n",
    "#plotting the top 5 platforms\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.pie(jp_platform_sales_top5, labels=jp_platform_sales_top5.index, radius=3)\n",
    "#plotting in pie chart ALL platforms\n",
    "\n",
    "plt.title(\"Top 5 Platforms in Japan since 2012 \")\n",
    "#naming the pie chart\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can identify that every region has different likes. For example North America has ds platform at 4th spot and Europe has it at 5th whereas Japan has it number 1. Additionally, Japan has 3ds platform at number 2 (most favorable) whereas North America and Europe don't even have it at their top 5 platforms. It's very interesting to see especially on all the pie charts how different the popularity of the platforms is per region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Profitable Genres per Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(na_region['na_sales'].describe())\n",
    "#getting a closer look at the exact number for na region\n",
    "\n",
    "print ()\n",
    "#printing an empty line\n",
    "\n",
    "print(eu_region['eu_sales'].describe())\n",
    "#getting a closer look at the exact number for eu region\n",
    "\n",
    "print ()\n",
    "#printing an empty line\n",
    "\n",
    "print(jp_region['jp_sales'].describe())\n",
    "#getting a closer look at the exact number for eu region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider all games that belong on the 4th quartile (so between 75% and max sales) to be considered popular and we will store this to a new variable for each region. More specifically for na will be any game with sales > 0.21M, for eu will be any game with sales > 0.11M and lastly for jp will be any game with sales > 0.03M."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Profitable Genres in North America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_popular_games = na_region[na_region['na_sales'] > 0.21]\n",
    "#storing all sales above 0.21M that were made in na to a new variable\n",
    "\n",
    "na_genre_count = na_popular_games.groupby('genre')['na_sales'].count().sort_values(ascending=False)\n",
    "#collecting all genres and their distribution\n",
    "\n",
    "na_popular_games_top5 = na_genre_count.head()\n",
    "#storing the top 5 game genres of na to a variable called na_popular_games_top5\n",
    "\n",
    "plt.pie(na_popular_games_top5, labels=na_popular_games_top5.index, radius=3)\n",
    "#plotting in pie chart ALL genres for the na region\n",
    "\n",
    "plt.title(\"Top 5 Games Genres in North America\")\n",
    "#naming the pie chart\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Profitable Genres in Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_popular_games = eu_region[eu_region['eu_sales'] > 0.11]\n",
    "#storing all sales above 0.11M that were made in eu to a new variable\n",
    "\n",
    "eu_genre_count = eu_popular_games.groupby('genre')['eu_sales'].count().sort_values(ascending=False)\n",
    "#collecting all genres and their distribution\n",
    "\n",
    "eu_popular_games_top5 = eu_genre_count.head()\n",
    "#storing the top 5 game genres of eu to a variable called eu_popular_games_top5\n",
    "\n",
    "plt.pie(eu_popular_games_top5, labels=eu_popular_games_top5.index, radius=3)\n",
    "#plotting in pie chart ALL genres for the eu region\n",
    "\n",
    "plt.title(\"Top 5 Game Genres in Europe\")\n",
    "#naming the pie chart\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Profitable Genres in Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_popular_games = jp_region[jp_region['jp_sales'] > 0.03]\n",
    "#storing all sales above 0.03M that were made in jp to a new variable\n",
    "\n",
    "jp_genre_count = jp_popular_games.groupby('genre')['jp_sales'].count().sort_values(ascending=False)\n",
    "#collecting all genres and their distribution\n",
    "\n",
    "jp_popular_games_top5 = jp_genre_count.head()\n",
    "#storing the top 5 game genres of jp to a variable called jp_popular_games_top5\n",
    "\n",
    "plt.pie(jp_popular_games_top5, labels=jp_popular_games_top5.index, radius=3)\n",
    "#plotting in pie chart ALL genres for the jp region\n",
    "\n",
    "plt.title(\"Top 5 Game Genres in Japan\")\n",
    "#naming the pie chart\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell that action genre dominates all regions but at the second most popular number of genre we have many differences. More specifically, at the North American market we have the sports genre as the second most popular genre, in Europe we have the shooter, and in Japan we have the role-playing genre. From the above statement we can tell that popular genders are not the same in each region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESRB ratings and Sales per Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESRB Ratings and Sales in North America "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_region.plot(x='rating', y='na_sales', kind='scatter', figsize=(15,5))\n",
    "#plotting a scatter for the na_region with x column rating and y column the na_sales, size 15 x 5 inches\n",
    "\n",
    "plt.title(\"ESRB ratings and Sales in North America\")\n",
    "#naming the pie chart\n",
    "\n",
    "plt.show()\n",
    "#showing the scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESRB Ratings and Sales in Europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_region.plot(x='rating', y='eu_sales', kind='scatter', figsize=(15,5))\n",
    "#plotting a scatter for the eu_region with x column rating and y column the na_sales, size 15 x 5 inches\n",
    "\n",
    "plt.title(\"ESRB ratings and Sales in Europe\")\n",
    "#naming the pie chart\n",
    "\n",
    "plt.show()\n",
    "#showing the scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESRB Ratings and Sales in Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp_region.plot(x='rating', y='jp_sales', kind='scatter', figsize=(15,5))\n",
    "#plotting a scatter for the jp_region with x column rating and y column the na_sales, size 15 x 5 inches\n",
    "\n",
    "plt.title(\"ESRB ratings and Sales in Japan\")\n",
    "#naming the pie chart\n",
    "\n",
    "plt.show()\n",
    "#showing the scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly identify that the ESRB ratings do affect sales in most regions. Taking a closer look in both the North American market and the European one we can tell that games with rating E and M have much higher sales than games with a rating of T, E10+,EC and RP. In Japan games with T or E rating have the most sales. We can conclude that ESRB ratings do affect the sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stating H0 and HA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start by specifying our null hypothesis and alternate hypothesis:\n",
    "\n",
    "Null Hypothesis: Average user ratings of the Xbox One and PC platforms are the same.\n",
    "\n",
    "Alternate Hypothesis: Average user ratings for the Action and Sports genres are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['user_score'] != -1]\n",
    "#making sure that we store all data that is not -1 to a new variable called 'filtered_userscore'\n",
    "#(since this is a value we created / unpragmatic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_data = filtered_df[filtered_df['platform'] == 'pc']\n",
    "#storing all pc data to a new variable called 'pc_data'\n",
    "\n",
    "xb_data = filtered_df[filtered_df['platform'] == 'xb']\n",
    "#storing all xb data to a new variable called 'xb_data'\n",
    "\n",
    "print(np.var(pc_data['user_score']))\n",
    "#finding the variance for the pc_data and user_score column\n",
    "\n",
    "print(np.var(xb_data['user_score']))\n",
    "#finding the variance for the xb_data and user_score column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance it looks like the variance are neither very close nor very far. Let's run the t-test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05 \n",
    "#creating our statistical significance at 5% \n",
    "\n",
    "results = st.ttest_ind(pc_data['user_score'], xb_data['user_score'], equal_var=False)\n",
    "#performing the t-test with the help of the scipy library\n",
    "\n",
    "print ('p-value:', results.pvalue) \n",
    "#printing the p-value\n",
    "\n",
    "if results.pvalue < alpha: \n",
    "#if p-value is less than alpha meaning the averages are not the same\n",
    "\n",
    "    print (\"We reject the null Hypothesis\") \n",
    "    #we won't reject the null hypothesis\n",
    "\n",
    "else:\n",
    "    print (\"We can't reject the null hypothesis\") \n",
    "    #we will reject the null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that we don't have enough evidence to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our project we started by \"cleaning\" our data column by column and we added a total_slaes column that would help our future analysis. Additionally, at the values of 'critic_score' and 'user_score' are very individual as every critic/user has own oppinion on the game. Thus, we updated all NaNs tbds with an unpragmatic value of -1. If we would have replaced with mean/median then our final result would not have been representative of our data. \n",
    "\n",
    "Later, we started our analysis by looking at the game releases per year, each platform performance and we concluded that our most relevant data to use for the purpose of our analysis would be the data after the year 2007. Once we added the relevant data to a new variable we analyzed the performance of each platform (with the new data), built a box plot for the global sales of all games, broken down by platform and discussed if the differences in sales and average were significant. Later, we took a look at how user and professional reviews affect sales for one popular platform (we chose specifically the ds platform). Additionally, we built a scatter plot and calculate the correlation between reviews and sales. Lastly, we focused on the general distribution of games by genre and commented upon the most profitable genres combined with creating a boxplot to reflect the information.\n",
    "\n",
    "Our next stop for our project was to create a user profile for each of the 3 regions that we had data for, North America, Europe and Japan. Later, we looked at the platform performance for each as well as the the most profitable genres per region. Lastly, we tried to see if the ESRB ratings afferct in any way the sales of the games, which we concluded that it does.\n",
    "\n",
    "We completed our project by testing the null hypothesis. Prior doing so though we filtered the data so that the  -1 (the unpragamtic value from the 'user_score' column) was excluded from our test hypothesis since it is not a value that reflects the real data. \n",
    "\n",
    "Average user ratings of the Xbox One and PC platforms are the same. We concluded that we don't have enough evidence to reject the null hypothesis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "289.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
